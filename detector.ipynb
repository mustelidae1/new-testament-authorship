{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk import NLP\n",
    "from cltk.data.fetch import FetchCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Tesserae Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äéê§Ä CLTK version '1.0.11'.\n",
      "Pipeline for language 'Ancient Greek' (ISO: 'grc'): `GreekNormalizeProcess`, `GreekStanzaProcess`, `GreekEmbeddingsProcess`, `StopsProcess`, `GreekNERProcess`.\n"
     ]
    }
   ],
   "source": [
    "corpus_downloader = FetchCorpus(language=\"grc\")\n",
    "corpus_downloader.import_corpus(\"grc_text_tesserae\")\n",
    "cltk_nlp = NLP(language='grc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Pauline from Non-Pauline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paul = [\n",
    "    # undisputed\n",
    "    'new_testament.i_thessalonians',\n",
    "    'new_testament.galatians',\n",
    "    'new_testament.i_corinthinians',\n",
    "    'new_testament.philippians',\n",
    "    'new_testament.philemon',\n",
    "    'new_testament.ii_corinthinians',\n",
    "    'new_testament.romans',\n",
    "    \n",
    "    # undecided\n",
    "    'new_testament.colossians',\n",
    "    'new_testament.ii_thessalonians',\n",
    "    \n",
    "    # disputed\n",
    "    'new_testament.ephesians',\n",
    "    'new_testament.i_timothy',\n",
    "    'new_testament.ii_timothy',\n",
    "    'new_testament.titus',\n",
    "    \n",
    "    # refuted\n",
    "    'new_testament.hebrews',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_paul = [\n",
    "   'new_testament.mathew', \n",
    "   'new_testament.mark', \n",
    "   'new_testament.luke',\n",
    "   'new_testament.john', \n",
    "   'new_testament.revelation', \n",
    "   'clement.exhortation', \n",
    "   'clement.protrepticus', \n",
    "   'demosthenes.letters', \n",
    "   'euripides.electra', \n",
    "   'plutarch.romulus', \n",
    "   'aeschylus.seven_against_thebes', \n",
    "   'appian.civil_wars.part.1', \n",
    "   'aristophanes.lysistrata', \n",
    "   'aristotle.nicomachean_ethics', \n",
    "   'basil_of_caesarea.de_legendis',\n",
    "   'flavius_josephus.antiquitates_judaicae.part.1', \n",
    "   'gregory_of_nazianzus.christus_patiens', \n",
    "   'herodotus.histories.part.7', \n",
    "   'homer.odyssey.part.16', \n",
    "   'new_testament.acts', \n",
    "   'plato.meno'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Corpus Directory From Local .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "DATA_DIR = os.getenv('DATA_DIR')\n",
    "files = os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dd(list)\n",
    "authors = dd(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file):\n",
    "    filepath = '{}/{}'.format(DATA_DIR, file)\n",
    "    text = re.sub('<[^<]+>', \"\", open(filepath, encoding=\"utf8\").read())\n",
    "    return re.sub('\\n', \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4519f63352004f779127cf3b66295b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for file in tqdm(files):\n",
    "    key = os.path.splitext(file)[0]\n",
    "    author = key.split('.')[0]\n",
    "    authors[author].append(key)\n",
    "    text = read_text(file)\n",
    "    texts[key] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_annotations(T):\n",
    "    D = dd(list)\n",
    "\n",
    "    for t in tqdm(T):\n",
    "        D[t] = cltk_nlp.analyze(text=T[t])\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## docs = get_annotations(paul + not_paul)\n",
    "## save_obj(docs, \"docs\")\n",
    "\n",
    "docs = load_obj(\"docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(doc, i, n):\n",
    "    if i > n - 1:\n",
    "        return None\n",
    "    return doc[i]\n",
    "\n",
    "def get_bigram(W):\n",
    "    if W[1] is None:\n",
    "        return []\n",
    "    \n",
    "    w_gram = \"{} {}\".format(W[0].string, W[1].string)\n",
    "    p_gram = \"{} {}\".format(W[0].upos, W[1].upos)\n",
    "    \n",
    "    return [w_gram, p_gram]\n",
    "\n",
    "def get_trigram(W):\n",
    "    if W[2] is None:\n",
    "        return []\n",
    "    \n",
    "    w_gram = \"{} {} {}\".format(W[0].string, W[1].string, W[2].string)\n",
    "    p_gram = \"{} {} {}\".format(W[0].upos, W[1].upos, W[2].upos)\n",
    "    \n",
    "    return [w_gram, p_gram]\n",
    "\n",
    "def increment_features(F, V):\n",
    "    for i,k in enumerate(F):\n",
    "        if i > len(V) - 1:\n",
    "            break\n",
    "        F[k][V[i]] += 1\n",
    "    return F\n",
    "\n",
    "def get_percents(F, f, n):\n",
    "    for k, v in F[f].items():\n",
    "        F[f][k] = F[f][k] / n\n",
    "    return F[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(doc):\n",
    "    n = len(doc.words)\n",
    "    \n",
    "    features = {\n",
    "        'word_bigram': dd(int),\n",
    "        'word_trigram': dd(int),\n",
    "        'pos_bigram': dd(int),\n",
    "        'pos_trigram': dd(int)\n",
    "    }\n",
    "\n",
    "    for i, w in enumerate(doc):\n",
    "        w2 = get_word(doc, i+1, n)\n",
    "        w3 = get_word(doc, i+2, n)\n",
    "        \n",
    "        bigrams = get_bigram([w, w2])\n",
    "        trigrams = get_trigram([w, w2, w3])\n",
    "        n_grams = bigrams + trigrams\n",
    "        features = increment_features(features, n_grams)\n",
    "        \n",
    "    for f in features:\n",
    "        features[f] = get_percents(features, f, n)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features From Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATS = [0, 1]\n",
    "\n",
    "data = {}\n",
    "\n",
    "for doc in docs:\n",
    "    features = get_features(docs[doc])\n",
    "    \n",
    "    for key in features:\n",
    "        features[key] = sorted(features[key].items(), key=lambda item: item[1])\n",
    "        \n",
    "    data[doc] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from more_itertools import take\n",
    "\n",
    "def topN(row, n):\n",
    "    x = row.to_dict() # convert the input row to a dictionary \n",
    "    x = {k: v for k, v in sorted(x.items(), key=lambda item: -item[1])} # sort the dictionary based on their values \n",
    "    n_items = take(n, x.items()) # extract the first n values from the dictionary \n",
    "    return n_items\n",
    "\n",
    "def get_df(d, c, n):\n",
    "    df = pd.DataFrame(index=[0]) \n",
    "    for f in features:\n",
    "        for x in data[d][f]:\n",
    "            df[x[0]] = x[1]\n",
    "    topColsTuples = topN(df.iloc[0], n)\n",
    "    topColsNames = [ entry[0] for entry in topColsTuples ]\n",
    "    finalDf = df[topColsNames]        \n",
    "    finalDf.insert(0, 'cat', c)        \n",
    "    finalDf.insert(0, 'text', d) \n",
    "    return finalDf\n",
    "\n",
    "def get_pos_df(D, n):\n",
    "    df = pd.DataFrame()\n",
    "    for d in tqdm(D):\n",
    "        df = df.append(get_df(d, CATS[1], n), ignore_index=True)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "def get_neg_df(D, E, n):\n",
    "    df = pd.DataFrame()\n",
    "    for d in tqdm(D):\n",
    "        if d not in E:\n",
    "            df = df.append(get_df(d, CATS[0], n), ignore_index=True)\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668ccc11e87944f58a61d4985480ad3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8cd34bec1a4527a2523fe84e5c546c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#n = 10 # number of highest entries to include \n",
    "\n",
    "#pos_df = get_pos_df(paul[:6], n)\n",
    "#neg_df = get_neg_df(docs, paul[:6], n)\n",
    "\n",
    "#df = pos_df.append(neg_df, ignore_index=True)\n",
    "#df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_obj(df, \"df\")\n",
    "df = load_obj(\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    c = df.copy()\n",
    "    \n",
    "    test = c.sample(frac=0.3, random_state=300)\n",
    "    train = c.drop(test.index)\n",
    "\n",
    "    test_y = test.cat.values\n",
    "    train_y = train.cat.values\n",
    "\n",
    "    train.drop(train.columns[[0, 1]], axis=1, inplace=True)\n",
    "    test.drop(test.columns[[0, 1]], axis=1, inplace=True)\n",
    "\n",
    "    train_X = np.array(train.values) \n",
    "    test_X = np.array(test.values)\n",
    "    \n",
    "    return (train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "PROBABILITIES:\n",
      "[[0.75503342 0.24496658]\n",
      " [0.75647066 0.24352934]\n",
      " [0.75466475 0.24533525]\n",
      " [0.75424633 0.24575367]\n",
      " [0.75414557 0.24585443]\n",
      " [0.75435813 0.24564187]\n",
      " [0.75467746 0.24532254]\n",
      " [0.75470133 0.24529867]\n",
      " [0.75379878 0.24620122]\n",
      " [0.75578804 0.24421196]]\n"
     ]
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression(solver='liblinear')\n",
    "logisticRegr.fit(train_X, train_y) \n",
    "\n",
    "result = logisticRegr.predict(test_X)\n",
    "prob = logisticRegr.predict_proba(test_X)\n",
    "\n",
    "print('PREDICTION:\\n{}\\n'.format(result))\n",
    "print('PROBABILITIES:\\n{}'.format(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = 0 \n",
    "\n",
    "for i, label in enumerate(test_y):\n",
    "    if result[i] == label:\n",
    "        accuracy_score = accuracy_score + 1 \n",
    "        \n",
    "print('ACCURACY: {}'.format(accuracy_score / len(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.8221 - accuracy: 0.2000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8029 - accuracy: 0.2000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8160 - accuracy: 0.2000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7966 - accuracy: 0.2000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8307 - accuracy: 0.2000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8013 - accuracy: 0.2000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8039 - accuracy: 0.2000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7935 - accuracy: 0.2000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7809 - accuracy: 0.2000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7944 - accuracy: 0.2000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8055 - accuracy: 0.2000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7784 - accuracy: 0.2000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8293 - accuracy: 0.2000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7937 - accuracy: 0.2000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8039 - accuracy: 0.2000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7947 - accuracy: 0.2000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8127 - accuracy: 0.2000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8317 - accuracy: 0.2000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8176 - accuracy: 0.2000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8001 - accuracy: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21d14b1fc70>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_X\n",
    "y = train_y.reshape((-1,1))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(20, activation='sigmoid'),\n",
    "    tf.keras.layers.Dropout(.1),\n",
    "    tf.keras.layers.Dense(5, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(len(CATS))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X, y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:greek]",
   "language": "python",
   "name": "conda-env-greek-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
