{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'gensim._matutils.array' has no attribute '__reduce_cython__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-084861810050>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNLP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFetchCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\greek\\lib\\site-packages\\cltk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNLP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ = curr_version = pkg_resources.get_distribution(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\greek\\lib\\site-packages\\cltk\\nlp.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_types\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mProcess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUnimplementedAlgorithmError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m from cltk.languages.pipelines import (\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mAkkadianPipeline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mArabicPipeline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\greek\\lib\\site-packages\\cltk\\languages\\pipelines.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mOldFrenchStanzaProcess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[1;33m from cltk.embeddings.processes import (\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mArabicEmbeddingsProcess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mAramaicEmbeddingsProcess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\greek\\lib\\site-packages\\cltk\\embeddings\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"Init for ``cltk.embeddings``.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\greek\\lib\\site-packages\\cltk\\embeddings\\embeddings.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCLTKException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnimplementedAlgorithmError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\greek\\lib\\site-packages\\gensim\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\greek\\lib\\site-packages\\gensim\\corpora\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[1;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\greek\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\greek\\lib\\site-packages\\gensim\\interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\greek\\lib\\site-packages\\gensim\\matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m     \u001b[1;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\greek\\lib\\site-packages\\gensim\\_matutils.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'gensim._matutils.array' has no attribute '__reduce_cython__'"
     ]
    }
   ],
   "source": [
    "from cltk import NLP\n",
    "from cltk.data.fetch import FetchCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Tesserae Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_downloader = FetchCorpus(language=\"grc\")\n",
    "#corpus_downloader.import_corpus(\"grc_text_tesserae\")\n",
    "cltk_nlp = NLP(language='grc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paul's Epistles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paul = [\n",
    "    # undisputed\n",
    "    'new_testament.i_thessalonians',\n",
    "    'new_testament.galatians',\n",
    "    'new_testament.i_corinthians',\n",
    "    'new_testament.philippians',\n",
    "    'new_testament.philemon',\n",
    "    'new_testament.ii_corinthians',\n",
    "    'new_testament.romans',\n",
    "    \n",
    "    # undecided\n",
    "    'new_testament.colossians',\n",
    "    'new_testament.ii_thessalonians',\n",
    "    \n",
    "    # disputed\n",
    "    'new_testament.ephesians',\n",
    "    'new_testament.i_timothy',\n",
    "    'new_testament.ii_timothy',\n",
    "    'new_testament.titus',\n",
    "    \n",
    "    # refuted\n",
    "    'new_testament.hebrews',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Corpus Directory From Local .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "DATA_DIR = os.getenv('DATA_DIR')\n",
    "files = os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "texts = dd(list)\n",
    "authors = dd(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file):\n",
    "    filepath = '{}/{}'.format(DATA_DIR, file)\n",
    "    text = re.sub('<[^<]+>', \"\", open(filepath, encoding=\"utf8\").read())\n",
    "    return re.sub('\\n', \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(files):\n",
    "    key = os.path.splitext(file)[0]\n",
    "    author = key.split('.')[0]\n",
    "    authors[author].append(key)\n",
    "    text = read_text(file)\n",
    "    texts[key] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dd(list)\n",
    "for key in tqdm(paul[:1]): # train \n",
    "    docs[key] = cltk_nlp.analyze(text=texts[key])\n",
    "    \n",
    "for key in tqdm(paul[3:4]): # train \n",
    "    docs[key] = cltk_nlp.analyze(text=texts[key])\n",
    "    \n",
    "for key in tqdm(paul[1:2]): # test\n",
    "    docs[key] = cltk_nlp.analyze(text=texts[key])\n",
    "    \n",
    "for key in tqdm(authors['plato'][:1]): # predict\n",
    "    docs[key] = cltk_nlp.analyze(text=texts[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Doc Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(doc):\n",
    "    features = {\n",
    "        'word_bigram': dd(int),\n",
    "        'word_trigram': dd(int),\n",
    "        'pos_bigram': dd(int),\n",
    "        'pos_trigram': dd(int)\n",
    "    }\n",
    "    \n",
    "    for i, word in enumerate(doc):        \n",
    "        if (i+1 > len(doc.words)-1):\n",
    "            break  \n",
    "        word_bigram = \"{word_one} {word_two}\".format(word_one = doc[i].string, word_two = doc[i+1].string)   \n",
    "        features['word_bigram'][word_bigram] += 1 \n",
    "        pos_bigram = \"{word_one} {word_two}\".format(word_one = doc[i].upos, word_two = doc[i+1].upos)  \n",
    "        features['pos_bigram'][pos_bigram] += 1 \n",
    "        \n",
    "        if (i+2 > len(doc.words)-1):\n",
    "            break  \n",
    "        word_trigram = \"{word_one} {word_two} {word_three}\".format(word_one = doc[i].string, word_two = doc[i+1].string, word_three = doc[i+2].string)  \n",
    "        features['word_trigram'][word_trigram] += 1 \n",
    "        pos_trigram = \"{word_one} {word_two} {word_three}\".format(word_one = doc[i].upos, word_two = doc[i+1].upos, word_three = doc[i+2].upos)\n",
    "        features['pos_trigram'][pos_trigram] += 1 \n",
    "    \n",
    "    for key, value in features['word_bigram'].items():\n",
    "        features['word_bigram'][key] = features['word_bigram'][key] / len(doc.words)\n",
    "    for key, value in features['word_trigram'].items():\n",
    "        features['word_trigram'][key] = features['word_trigram'][key] / len(doc.words)\n",
    "    for key, value in features['pos_bigram'].items():\n",
    "        features['pos_bigram'][key] = features['pos_bigram'][key] / len(doc.words)\n",
    "    for key, value in features['pos_trigram'].items():\n",
    "        features['pos_trigram'][key] = features['pos_trigram'][key] / len(doc.words)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features From Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for doc in docs:\n",
    "    features = get_features(docs[doc])\n",
    "    for key in features:\n",
    "        features[key] = sorted(features[key].items(), key=lambda item: item[1])\n",
    "    data[doc] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suffix Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystlm.stlm import STLM\n",
    "from pystlm.suffixtree import SuffixTree\n",
    "from pystlm.sequence import Sequence\n",
    "\n",
    "trie = SuffixTree()\n",
    "\n",
    "doc = docs[paul[0]]\n",
    "\n",
    "tags = []\n",
    "\n",
    "for word in doc:\n",
    "    tags.append(word.upos)\n",
    "    trie.add(word.upos)\n",
    "\n",
    "trie.update_all_counts()\n",
    "stlm = STLM(trie)\n",
    "seq = Sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sentences = doc.sentences\n",
    "sentence_tags = []\n",
    "\n",
    "start_counter = Counter()\n",
    "end_counter = Counter()\n",
    "\n",
    "for sent in sentences:\n",
    "    if len(sent) < 2: continue\n",
    "    pos_tags = []\n",
    "    for word in sent:\n",
    "        pos_tags.append(word.upos)\n",
    "    sentence_tags.append(pos_tags)\n",
    "    \n",
    "    start_counter[pos_tags[0]] += 1\n",
    "    end_counter[pos_tags[len(pos_tags)-1]] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in data.items():\n",
    "    print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['new_testament.i_thessalonians']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "cats = [0, 1]\n",
    "\n",
    "# training data \n",
    "df = pd.DataFrame({'cat': 1}, index=[0])\n",
    "\n",
    "data_train_1 = pd.DataFrame({'cat': 1}, index=[0])\n",
    "data_train_1['text'] = 'new_testament.i_thessalonians'\n",
    "for bigram in data['new_testament.i_thessalonians']['pos_bigram']: \n",
    "    data_train_1[bigram[0]] = bigram[1]\n",
    "for bigram in data['new_testament.i_thessalonians']['word_bigram']: \n",
    "    data_train_1[bigram[0]] = bigram[1]\n",
    "for trigram in data['new_testament.i_thessalonians']['pos_trigram']: \n",
    "    data_train_1[trigram[0]] = trigram[1]\n",
    "for trigram in data['new_testament.i_thessalonians']['word_trigram']: \n",
    "    data_train_1[trigram[0]] = trigram[1]\n",
    "    \n",
    "data_train_2 = pd.DataFrame({'cat': 1}, index=[0])\n",
    "data_train_2['text'] = 'new_testament.philippians'\n",
    "for bigram in data['new_testament.philippians']['pos_bigram']: \n",
    "    data_train_2[bigram[0]] = bigram[1]\n",
    "for bigram in data['new_testament.philippians']['word_bigram']: \n",
    "    data_train_2[bigram[0]] = bigram[1]\n",
    "for trigram in data['new_testament.philippians']['pos_trigram']: \n",
    "    data_train_2[trigram[0]] = trigram[1]\n",
    "for trigram in data['new_testament.philippians']['word_trigram']: \n",
    "    data_train_2[trigram[0]] = trigram[1]\n",
    "    \n",
    "data_train_3 = pd.DataFrame({'cat': 0}, index=[0])\n",
    "data_train_3['text'] = 'plato.alcibiades_1'\n",
    "for bigram in data['plato.alcibiades_1']['pos_bigram']: \n",
    "    data_train_3[bigram[0]] = bigram[1]\n",
    "for bigram in data['plato.alcibiades_1']['word_bigram']: \n",
    "    data_train_3[bigram[0]] = bigram[1]\n",
    "for trigram in data['plato.alcibiades_1']['pos_trigram']: \n",
    "    data_train_3[trigram[0]] = trigram[1]\n",
    "for trigram in data['plato.alcibiades_1']['word_trigram']: \n",
    "    data_train_3[trigram[0]] = trigram[1]\n",
    "    \n",
    "# test data \n",
    "data_test = pd.DataFrame({'cat': 1}, index=[0])\n",
    "data_test['text'] = 'new_testament.galatians'\n",
    "for bigram in data['new_testament.galatians']['pos_bigram']: \n",
    "    data_test[bigram[0]] = bigram[1]\n",
    "for bigram in data['new_testament.galatians']['word_bigram']: \n",
    "    data_test[bigram[0]] = bigram[1]\n",
    "for trigram in data['new_testament.galatians']['pos_trigram']: \n",
    "    data_test[trigram[0]] = trigram[1]\n",
    "for trigram in data['new_testament.galatians']['word_trigram']: \n",
    "    data_test[trigram[0]] = trigram[1]\n",
    "    \n",
    "df = df.append(data_train_1, ignore_index = True)\n",
    "df = df.append(data_train_2, ignore_index = True)\n",
    "df = df.append(data_train_3, ignore_index = True)\n",
    "df = df.append(data_test, ignore_index = True)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = df.loc[df['text'] == 'new_testament.i_thessalonians' || df['text'] == 'new_testament.philippians' || df['text'] == 'plato.alcibiades_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = df.loc[df['text'] == 'new_testament.i_thessalonians' || df['text'] == 'new_testament.philippians' || df['text'] == 'plato.alcibiades_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "test = data_test\n",
    "train = data_train\n",
    "\n",
    "train_y = train.cat.values\n",
    "test_y = test.cat.values\n",
    "\n",
    "data_train.drop(data_test.columns[[0]], axis=1, inplace=True)\n",
    "data_test.drop(data_test.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "train_X = np.array(data_train.values) \n",
    "test_X = np.array(data_test.values) \n",
    "\n",
    "print(train_X.shape, train_y.shape)\n",
    "print(test_X.shape, test_y.shape)\n",
    "\n",
    "print(train_X)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logisticRegr = LogisticRegression(solver='liblinear')\n",
    "logisticRegr.fit(train_X, train_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=300, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=600, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=300, activation='relu'),\n",
    "    tf.keras.layers.Dropout(.1),\n",
    "    tf.keras.layers.Dense(units=len(cats), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_X, train_y, epochs=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:greek]",
   "language": "python",
   "name": "conda-env-greek-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
