{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk import NLP\n",
    "from cltk.data.fetch import FetchCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Tesserae Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äéê§Ä CLTK version '1.0.6'.\n",
      "Pipeline for language 'Ancient Greek' (ISO: 'grc'): `GreekNormalizeProcess`, `GreekStanzaProcess`, `GreekEmbeddingsProcess`, `StopsProcess`, `GreekNERProcess`.\n"
     ]
    }
   ],
   "source": [
    "corpus_downloader = FetchCorpus(language=\"grc\")\n",
    "corpus_downloader.import_corpus(\"grc_text_tesserae\")\n",
    "cltk_nlp = NLP(language='grc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Pauline from Non-Pauline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paul = [\n",
    "    # undisputed\n",
    "    'new_testament.i_thessalonians',\n",
    "    'new_testament.galatians',\n",
    "    'new_testament.i_corinthinians',\n",
    "    'new_testament.philippians',\n",
    "    'new_testament.philemon',\n",
    "    'new_testament.ii_corinthinians',\n",
    "    'new_testament.romans',\n",
    "    \n",
    "    # undecided\n",
    "    'new_testament.colossians',\n",
    "    'new_testament.ii_thessalonians',\n",
    "    \n",
    "    # disputed\n",
    "    'new_testament.ephesians',\n",
    "    'new_testament.i_timothy',\n",
    "    'new_testament.ii_timothy',\n",
    "    'new_testament.titus',\n",
    "    \n",
    "    # refuted\n",
    "    'new_testament.hebrews',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_paul = [\n",
    "   'new_testament.mathew', \n",
    "   'new_testament.mark', \n",
    "   'new_testament.luke',\n",
    "   'new_testament.john', \n",
    "   'new_testament.revelation', \n",
    "   'clement.exhortation', \n",
    "   'clement.protrepticus', \n",
    "   'demosthenes.letters', \n",
    "   'euripides.electra', \n",
    "   'plutarch.romulus', \n",
    "   'aeschylus.seven_against_thebes', \n",
    "   'appian.civil_wars.part.1', \n",
    "   'aristophanes.lysistrata', \n",
    "   'aristotle.nicomachean_ethics', \n",
    "   'basil_of_caesarea.de_legendis',\n",
    "   'flavius_josephus.antiquitates_judaicae.part.1', \n",
    "   'gregory_of_nazianzus.christus_patiens', \n",
    "   'herodotus.histories.part.7', \n",
    "   'homer.odyssey.part.16', \n",
    "   'new_testament.acts', \n",
    "   'plato.meno'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Corpus Directory From Local .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "DATA_DIR = os.getenv('DATA_DIR')\n",
    "files = os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dd(list)\n",
    "authors = dd(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file):\n",
    "    filepath = '{}/{}'.format(DATA_DIR, file)\n",
    "    text = re.sub('<[^<]+>', \"\", open(filepath, encoding=\"utf8\").read())\n",
    "    return re.sub('\\n', \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3172aed311e043148743b7de67161cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for file in tqdm(files):\n",
    "    key = os.path.splitext(file)[0]\n",
    "    author = key.split('.')[0]\n",
    "    authors[author].append(key)\n",
    "    text = read_text(file)\n",
    "    texts[key] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_annotations(T):\n",
    "    D = dd(list)\n",
    "\n",
    "    for t in tqdm(T):\n",
    "        D[t] = cltk_nlp.analyze(text=T[t])\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## docs = get_annotations(paul + not_paul)\n",
    "## save_obj(docs, \"docs\")\n",
    "\n",
    "docs = load_obj(\"docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(doc, i, n):\n",
    "    if i > n - 1:\n",
    "        return None\n",
    "    return doc[i]\n",
    "\n",
    "def get_bigram(W):\n",
    "    if W[1] is None:\n",
    "        return []\n",
    "    \n",
    "    w_gram = \"{} {}\".format(W[0].string, W[1].string)\n",
    "    p_gram = \"{} {}\".format(W[0].upos, W[1].upos)\n",
    "    \n",
    "    return [w_gram, p_gram]\n",
    "\n",
    "def get_trigram(W):\n",
    "    if W[2] is None:\n",
    "        return []\n",
    "    \n",
    "    w_gram = \"{} {} {}\".format(W[0].string, W[1].string, W[2].string)\n",
    "    p_gram = \"{} {} {}\".format(W[0].upos, W[1].upos, W[2].upos)\n",
    "    \n",
    "    return [w_gram, p_gram]\n",
    "\n",
    "def increment_features(F, V):\n",
    "    for i,k in enumerate(F):\n",
    "        if i > len(V) - 1:\n",
    "            break\n",
    "        F[k][V[i]] += 1\n",
    "    return F\n",
    "\n",
    "def get_percents(F, f, n):\n",
    "    for k, v in F[f].items():\n",
    "        F[f][k] = F[f][k] / n\n",
    "    return F[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(doc):\n",
    "    n = len(doc.words)\n",
    "    \n",
    "    features = {\n",
    "        'word_bigram': dd(int),\n",
    "        'word_trigram': dd(int),\n",
    "        'pos_bigram': dd(int),\n",
    "        'pos_trigram': dd(int)\n",
    "    }\n",
    "\n",
    "    for i, w in enumerate(doc):\n",
    "        w2 = get_word(doc, i+1, n)\n",
    "        w3 = get_word(doc, i+2, n)\n",
    "        \n",
    "        bigrams = get_bigram([w, w2])\n",
    "        trigrams = get_trigram([w, w2, w3])\n",
    "        n_grams = bigrams + trigrams\n",
    "        features = increment_features(features, n_grams)\n",
    "        \n",
    "    for f in features:\n",
    "        features[f] = get_percents(features, f, n)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features From Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATS = [0, 1]\n",
    "\n",
    "data = {}\n",
    "\n",
    "for doc in docs:\n",
    "    features = get_features(docs[doc])\n",
    "    \n",
    "    for key in features:\n",
    "        features[key] = sorted(features[key].items(), key=lambda item: item[1])\n",
    "        \n",
    "    data[doc] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "def get_df(d, c):\n",
    "    df = pd.DataFrame({'text': d, 'cat': c}, index=[0])\n",
    "    for f in features:\n",
    "        for x in data[d][f]:\n",
    "            df[x[0]] = x[1]\n",
    "    return df\n",
    "\n",
    "def get_pos_df(D):\n",
    "    df = pd.DataFrame()\n",
    "    for d in tqdm(D):\n",
    "        df = df.append(get_df(d, CATS[1]), ignore_index=True)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "def get_neg_df(D, E):\n",
    "    df = pd.DataFrame()\n",
    "    for d in tqdm(D):\n",
    "        if d not in E:\n",
    "            df = df.append(get_df(d, CATS[0]), ignore_index=True)\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3099547721fa48728f2e9731ed4d336d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e829ff371a0342ac966eed8cfce5eda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_df = get_pos_df(paul[:6])\n",
    "neg_df = get_neg_df(docs, paul[:6])\n",
    "\n",
    "df = pos_df.append(neg_df, ignore_index=True)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    c = df.copy()\n",
    "    \n",
    "    test = c.sample(frac=0.3, random_state=300)\n",
    "    train = c.drop(data_test.index)\n",
    "\n",
    "    test_y = test.cat.values\n",
    "    train_y = train.cat.values\n",
    "\n",
    "    train.drop(train.columns[[0, 1]], axis=1, inplace=True)\n",
    "    test.drop(test.columns[[0, 1]], axis=1, inplace=True)\n",
    "\n",
    "    train_X = np.array(train.values) \n",
    "    test_X = np.array(test.values)\n",
    "    \n",
    "    return (train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "PROBABILITIES:\n",
      "[[0.75444266 0.24555734]\n",
      " [0.75691738 0.24308262]\n",
      " [0.75526826 0.24473174]\n",
      " [0.7548589  0.2451411 ]\n",
      " [0.75474748 0.24525252]\n",
      " [0.75497662 0.24502338]\n",
      " [0.75573902 0.24426098]\n",
      " [0.75508164 0.24491836]\n",
      " [0.7538342  0.2461658 ]\n",
      " [0.75602899 0.24397101]]\n"
     ]
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression(solver='liblinear')\n",
    "logisticRegr.fit(train_X, train_y) \n",
    "\n",
    "result = logisticRegr.predict(test_X)\n",
    "prob = logisticRegr.predict_proba(test_X)\n",
    "\n",
    "print('PREDICTION:\\n{}\\n'.format(result))\n",
    "print('PROBABILITIES:\\n{}'.format(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = 0 \n",
    "\n",
    "for i, label in enumerate(test_y):\n",
    "    if result[i] == label:\n",
    "        accuracy_score = accuracy_score + 1 \n",
    "        \n",
    "print('ACCURACY: {}'.format(accuracy_score / len(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 527ms/step - loss: 1.8899 - accuracy: 0.8000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.8972 - accuracy: 0.8000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.8787 - accuracy: 0.8000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.8810 - accuracy: 0.8000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.6710 - accuracy: 0.8000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.8639 - accuracy: 0.8000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.8725 - accuracy: 0.8000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.6453 - accuracy: 0.8000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.6580 - accuracy: 0.8000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.6060 - accuracy: 0.8000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.3817 - accuracy: 0.8000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2295 - accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.5316 - accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.4055 - accuracy: 0.8000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.4417 - accuracy: 0.8000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.1632 - accuracy: 0.8000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1604 - accuracy: 0.8000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.3376 - accuracy: 0.8000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.4139 - accuracy: 0.8000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.9209 - accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3987688310>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_X\n",
    "y = train_y.reshape((-1,1))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(20, activation='sigmoid'),\n",
    "    tf.keras.layers.Dropout(.1),\n",
    "    tf.keras.layers.Dense(5, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(len(CATS))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X, y, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
